# app.py
import streamlit as st
import os
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --- Streamlit Page ---
st.set_page_config(page_title="PDF RAG Assistant", page_icon="ðŸ“„")
st.title("ðŸ“„ PDF RAG Assistant")
st.markdown(
    "Upload a PDF and ask questions about it. "
    "The assistant provides practical suggestions and insights using a RAG pipeline."
)

# --- Set OpenAI API Key from Streamlit secrets ---
if "OPENAI_API_KEY" not in st.secrets:
    st.error("Please set your OpenAI API key in Streamlit Secrets!")
    st.stop()
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# --- Upload PDF ---
uploaded_file = st.file_uploader("Upload your PDF", type=["pdf"])

if uploaded_file:
    st.info("Processing PDF... This may take a few seconds.")

    # Save uploaded file to a temporary file
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getbuffer())
        temp_file_path = tmp_file.name

    # Load & split PDF
    loader = PyPDFLoader(temp_file_path)
    docs = loader.load()
    splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(docs)

    # Initialize Chroma vector DB
    if "db" not in st.session_state:
        st.session_state["db"] = Chroma.from_documents(splits, OpenAIEmbeddings())

    db = st.session_state["db"]

    # Define prompt template
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""You are an assistant helping improve documents.
Based on the context, suggest what could be implemented, added, or improved.
Be practical and concise (max 3 suggestions).

Context:
{context}

Question: {question}
Answer:"""
    )

    # Initialize QA chain
    if "qa" not in st.session_state:
        st.session_state["qa"] = RetrievalQA.from_chain_type(
            llm=ChatOpenAI(model="gpt-3.5-turbo", temperature=0),
            retriever=db.as_retriever(),
            chain_type="stuff",
            chain_type_kwargs={"prompt": prompt},
        )

    qa = st.session_state["qa"]

    # --- User Question ---
    question = st.text_input("Ask a question about your PDF:")

    if question:
        with st.spinner("Generating answer..."):
            answer = qa.run(question)
            st.markdown(f"**Answer:** {answer}")
